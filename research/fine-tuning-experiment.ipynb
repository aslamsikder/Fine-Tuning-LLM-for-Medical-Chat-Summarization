{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309d1de7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fe87f8a",
   "metadata": {},
   "source": [
    "**Please run this notebook using Kaggle, there you will get free GPU T4 √ó 2.**\n",
    "\n",
    "See my project in Github: [Fine-Tuning LLM for Medical Chat Summarization](https://github.com/aslamsikder/Fine-Tuning-LLM-for-Medical-Chat-Summarization)  \n",
    "See my Kaggle Notebook: [Fine-Tuning LLM for Medical Chat Summarization](https://www.kaggle.com/code/aslamsikder/lora-fine-tuning-gemma-2b-medical-summarization?scriptVersionId=270967244)  \n",
    "\n",
    "‚úçÔ∏è Author Information\n",
    "Developed by **Aslam Sikder**, October 2025  \n",
    "Email: [aslamsikder.edu@gmail.com](mailto:aslamsikder.edu@gmail.com)  \n",
    "LinkedIn: [Aslam Sikder - Linkedin](https://www.linkedin.com/in/aslamsikder)  \n",
    "Google Scholar: [Aslam Sikder - Google Scholar](https://scholar.google.com/citations?hl=en&user=Ip1qQi8AAAAJ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f306752",
   "metadata": {},
   "source": [
    "**Cell 1: Project Setup and Installation**\n",
    "\n",
    "This first step is crucial for setting up our environment. We will install all the necessary Python libraries. The most important library here is unsloth, which is specifically designed to make fine-tuning large language models (LLMs) significantly faster and more memory-efficient. This library is the key to making this project feasible on a free, resource-constrained GPU (like a T4 in Google Colab / T4*2 in Kaggle) which typically has VRAM limitations that would prevent standard fine-tuning. We also install trl for its specialized training tools, peft for parameter-efficient fine-tuning, and other standard libraries from the Hugging Face ecosystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8a6ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1\n",
    "# Install required libraries\n",
    "# We use unsloth for memory-efficient and faster fine-tuning of LLMs.\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" -q\n",
    "!pip install \"trl<0.9.0\" peft accelerate bitsandbytes -q\n",
    "!pip install datasets evaluate bert_score rouge_score -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99617baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required library\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from huggingface_hub import login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "from flask import Flask, request, jsonify\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "import shutil\n",
    "import os\n",
    "import re "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169ea9bb",
   "metadata": {},
   "source": [
    "**Cell 2: Hugging Face Authentication**\n",
    "\n",
    "To download the model or dataset we need to authenticate with Hugging Face. This step uses the login function to securely connect to your Hugging Face account. You will need to generate an access token with \"write\" permissions from your Hugging Face account settings and save it as a secret in your environment (in Colab/Kaggle, this is done via the \"Secrets\" tab). This ensures your credentials are not exposed in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7eb182a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2\n",
    "# Hugging Face login & Git LFS setup (for Kaggle)\n",
    "\n",
    "# Authenticate with Hugging Face to access the Llama 3 model\n",
    "# Ensure you have saved your Hugging Face token as 'HF_TOKEN' in Colab secrets\n",
    "try:\n",
    "    secret_label = \"HF_TOKEN\"  # This should match the name of your secret in Kaggle\n",
    "    hf_token = UserSecretsClient().get_secret(secret_label)\n",
    "    \n",
    "    # Log in securely to Hugging Face\n",
    "    login(token=hf_token)\n",
    "    print(\"Successfully logged into Hugging Face.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not log in. Please ensure 'HF_TOKEN' is set in Colab secrets. Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd2c339",
   "metadata": {},
   "source": [
    "**Cell 3: Load Model and Tokenizer**\n",
    "\n",
    "Here, we load the Gemma 2b model and its corresponding tokenizer. We use unsloth's **FastLanguageModel** class, which is a highly optimized wrapper around the standard Hugging Face model class.\n",
    "\n",
    "Key parameters used:\n",
    "*   model_name = \"unsloth/gemma-2b-it-bnb-4bit\": We use a version of Llama 3 that has been pre-quantized to 4-bit precision and optimized for Unsloth. This dramatically reduces the memory required to load the model.\n",
    "*   load_in_4bit = True: This argument explicitly tells the model to load the weights in 4-bit precision, which is the core of our memory-saving strategy.\n",
    "*   max_seq_length = 2048: We set a maximum sequence length to balance context understanding with memory constraints.\n",
    "\n",
    "This cell makes the impossible possible: loading a billion-parameter model into a GPU with less than 8GB of VRAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4af5989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3\n",
    "\n",
    "# Define model loading parameters\n",
    "max_seq_length = 2048  # Maximum sequence length\n",
    "dtype = None           # Unsloth will automatically choose the best dtype (bf16/fp16)\n",
    "load_in_4bit = True    # Load in 4-bit quantized precision for memory efficiency\n",
    "\n",
    "print(\"Loading Gemma 2B model and tokenizer with Unsloth...\")\n",
    "\n",
    "# Load the pre-quantized Gemma 2B instruction-tuned model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/gemma-2b-it-bnb-4bit\",  # Instruction-tuned, 4-bit version\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    trust_remote_code=True,  # Necessary for custom model implementations\n",
    ")\n",
    "\n",
    "# Verification\n",
    "print(\"Model and tokenizer loaded successfully.\")\n",
    "print(f\"Model Type: {model.config.model_type}\")\n",
    "print(f\"Max Seq Length: {max_seq_length}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bde1b66",
   "metadata": {},
   "source": [
    "**Cell 4: Load and Explore the Dataset**\n",
    "\n",
    "We load our medical dialogue dataset directly from the Hugging Face Hub. The dataset is conveniently split into train, validation, and test sets. We will use the datasets library for this. After loading, we'll inspect the data to understand its structure, confirming the presence of the dialogue and soap columns which will serve as our input and target, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eff6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "\n",
    "\n",
    "# Load the medical dialogue dataset from Hugging Face\n",
    "dataset_name = \"omi-health/medical-dialogue-to-soap-summary\"\n",
    "\n",
    "print(f\"Loading dataset: {dataset_name} ...\")\n",
    "\n",
    "# Load train, validation, and test splits\n",
    "train_dataset = load_dataset(dataset_name, split=\"train\")\n",
    "validation_dataset = load_dataset(dataset_name, split=\"validation\")\n",
    "test_dataset = load_dataset(dataset_name, split=\"test\")\n",
    "\n",
    "# Display dataset information\n",
    "print(\"\\nDataset loaded successfully!\")\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(validation_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Display column names\n",
    "print(\"\\nAvailable columns:\", train_dataset.column_names)\n",
    "\n",
    "# Show one formatted example (to verify fields before preprocessing)\n",
    "print(\"\\nExample sample from training set:\")\n",
    "print(train_dataset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a98bd8",
   "metadata": {},
   "source": [
    "**Cell 5: Data Preprocessing and Prompt Formatting**\n",
    "\n",
    "Instruction-tuned models like Gemma 2B perform best when the input data is formatted as a clear instruction. We use the official Gemma 2B chat template to structure our data in a conversational format that distinguishes between the user‚Äôs request and the model‚Äôs response. Each example is converted into a prompt where the user instructs the model to summarize a given medical dialogue into a SOAP note, and the model provides the corresponding summary. This ensures that the fine-tuned model learns to follow natural instructions effectively. We define a function format_chat_template to apply this transformation and then use the .map() method to process all splits of our dataset efficiently for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe58e5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5\n",
    "\n",
    "def format_chat_template(row):\n",
    "    # Gemma 2B expects simple role-based text prompts\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Summarize the following medical dialogue into a SOAP note:\\n\\n{row['dialogue']}\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"model\",\n",
    "            \"content\": f\"{row['soap']}\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Define the official Gemma chat template\n",
    "    tokenizer.chat_template = (\n",
    "        \"{% for message in messages %}\"\n",
    "        \"{% if message['role'] == 'user' %}\"\n",
    "        \"{{ '<bos><start_of_turn>user\\n' + message['content'] + '<end_of_turn>\\n' }}\"\n",
    "        \"{% elif message['role'] == 'model' %}\"\n",
    "        \"{{ '<start_of_turn>model\\n' + message['content'] + '<end_of_turn>' }}\"\n",
    "        \"{% endif %}\"\n",
    "        \"{% endfor %}\"\n",
    "    )\n",
    "    \n",
    "    # Apply template without tokenizing\n",
    "    return {\"text\": tokenizer.apply_chat_template(messages, tokenize=False)}\n",
    "\n",
    "# Apply to all splits\n",
    "train_dataset_formatted = train_dataset.map(format_chat_template)\n",
    "validation_dataset_formatted = validation_dataset.map(format_chat_template)\n",
    "test_dataset_formatted = test_dataset.map(format_chat_template)\n",
    "\n",
    "# Display a formatted example\n",
    "print(\"\\nFormatted prompt example:\")\n",
    "print(train_dataset_formatted[\"text\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7507b5d2",
   "metadata": {},
   "source": [
    "**Cell 6: Configure LoRA (Parameter-Efficient Fine-Tuning)**\n",
    "\n",
    "In this cell, we prepare the Gemma 2B model for LoRA-based fine-tuning. LoRA (Low-Rank Adaptation) allows us to update only a small subset of parameters instead of the full model, making fine-tuning memory- and compute-efficient, especially for large models like Gemma 2B.\n",
    "\n",
    "* r = 16: The rank of the low-rank matrices injected into the model. Higher rank allows more capacity for adaptation but increases memory usage.\n",
    "\n",
    "* lora_alpha = 32: Scaling factor that balances the contribution of LoRA updates to the original weights.\n",
    "\n",
    "* target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]: Specifies which layers of the model will receive LoRA adapters. These include attention projection layers and MLP layers, which are critical for capturing task-specific behavior.\n",
    "\n",
    "* lora_dropout = 0.05: Applies a small dropout to LoRA updates for stability and regularization. Can be set to 0 if desired.\n",
    "\n",
    "* bias = \"none\": Keeps the original biases frozen and only trains LoRA parameters.\n",
    "\n",
    "* use_gradient_checkpointing = \"unsloth\": Enables memory-efficient backpropagation by storing fewer intermediate activations. This is crucial for training large models with limited GPU memory.\n",
    "\n",
    "* random_state = 3407: Ensures reproducibility of LoRA initialization.\n",
    "\n",
    "* use_rslora = False: Optional feature for rank-stabilized LoRA; only set True if using RSLora.\n",
    "\n",
    "* loftq_config = None: Reserved for LoFTQ quantization; not needed in our current setup.\n",
    "\n",
    "Overall, this configuration allows us to adapt Gemma 2B to our task efficiently, keeping memory consumption low while still enabling the model to learn the SOAP note summarization task effectively.\n",
    "\n",
    "Output: Confirms that LoRA adapters are successfully applied to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35977a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6\n",
    "\n",
    "# Configure the model for LoRA (Parameter-Efficient Fine-Tuning)\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",   # Attention projections\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"       # MLP projections\n",
    "    ],\n",
    "    lora_dropout=0.05,  # Small dropout for stability (can set to 0 if needed)\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Memory-efficient training\n",
    "    random_state=3407,\n",
    "    use_rslora=False,   # Set True only if using rank-stabilized LoRA (RSLora)\n",
    "    loftq_config=None,  # Not needed unless using LoFTQ quantization\n",
    ")\n",
    "\n",
    "print(\"LoRA adapters successfully configured on the Gemma 2B model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289ebecf",
   "metadata": {},
   "source": [
    "**Cell 7: Define Training Arguments**\n",
    "\n",
    "We define the hyperparameters for our fine-tuning process using the TrainingArguments class from the Hugging Face Transformers library. These settings are optimized for Gemma 2B fine-tuning with LoRA and 4-bit quantization using Unsloth. Each parameter is carefully chosen to balance training stability, speed, and memory efficiency.\n",
    "\n",
    "* output_dir = \"./outputs\": Directory where model checkpoints and logs will be saved.\n",
    "\n",
    "* num_train_epochs = 1: A single epoch is often sufficient for instruction fine-tuning, as the goal is to align the model with task-specific behavior rather than train from scratch.\n",
    "\n",
    "* per_device_train_batch_size = 2: Small batch size ensures that the 4-bit quantized model fits comfortably within GPU memory limits.\n",
    "\n",
    "* gradient_accumulation_steps = 4: Accumulates gradients over 4 mini-batches before performing an optimization step. This effectively simulates a larger batch size of 2 √ó 4 = 8, enhancing training stability without extra memory usage.\n",
    "\n",
    "* warmup_steps = 5: Gradually increases the learning rate during the initial steps to prevent training instability and allow smoother convergence.\n",
    "\n",
    "* learning_rate = 2e-4: A standard and reliable learning rate for QLoRA-based fine-tuning of instruction-tuned models.\n",
    "\n",
    "* fp16 = not torch.cuda.is_bf16_supported(): Enables half-precision (16-bit) floating-point computation on GPUs that don‚Äôt support bf16, improving speed and reducing memory use.\n",
    "\n",
    "* bf16 = torch.cuda.is_bf16_supported(): Enables bf16 precision when supported (e.g., A100, H100 GPUs) for faster and more stable mixed-precision training.\n",
    "\n",
    "* optim = \"adamw_8bit\": Uses a memory-efficient 8-bit version of the AdamW optimizer, significantly reducing GPU memory consumption.\n",
    "\n",
    "* weight_decay = 0.01: Adds a small regularization term to prevent overfitting during fine-tuning.\n",
    "\n",
    "* lr_scheduler_type = \"linear\": Uses a linear learning rate schedule, which gradually decays the learning rate as training progresses.\n",
    "\n",
    "* logging_steps = 10: Logs training progress every 10 steps for easier monitoring.\n",
    "\n",
    "* report_to = \"none\": Disables third-party logging integrations like Weights & Biases for a cleaner local training setup.\n",
    "\n",
    "Overall, these configurations are ideal for parameter-efficient fine-tuning (PEFT) on limited GPU resources while maintaining strong model performance and stable convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facb45e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7\n",
    "\n",
    "\n",
    "\n",
    "# Define the training arguments for Gemma 2B LoRA fine-tuning\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./outputs\",              # Directory to save model checkpoints and logs\n",
    "    num_train_epochs=1,                  # Number of epochs (increase for full training)\n",
    "    per_device_train_batch_size=2,       # Small batch size for memory efficiency\n",
    "    gradient_accumulation_steps=4,       # Accumulate gradients to simulate larger batches\n",
    "    warmup_steps=5,                      # Steps for LR warm-up\n",
    "    learning_rate=2e-4,                  # Good default for LoRA + 4-bit\n",
    "    fp16=not torch.cuda.is_bf16_supported(),  # Use fp16 if bf16 not available\n",
    "    bf16=torch.cuda.is_bf16_supported(),      # Prefer bf16 on newer GPUs (A100, H100)\n",
    "    logging_steps=10,                    # Log every 10 steps\n",
    "    save_strategy=\"epoch\",               # Save checkpoints each epoch\n",
    "    save_total_limit=2,                  # Keep only last 2 checkpoints\n",
    "    optim=\"adamw_8bit\",                  # Memory-efficient optimizer from bitsandbytes\n",
    "    weight_decay=0.01,                   # Regularization\n",
    "    lr_scheduler_type=\"linear\",          # Linear LR schedule\n",
    "    seed=3407,                           # Reproducibility\n",
    "    report_to=\"none\",                    # Disable external logging (e.g., W&B)\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7edc41",
   "metadata": {},
   "source": [
    "**Cell 8: Initialize SFTTrainer and Start Fine-Tuning**\n",
    "\n",
    "In this cell, we initialize the SFTTrainer from the trl library to perform LoRA-based fine-tuning of Gemma 2B on our SOAP summarization task. This trainer handles the complete training loop, including gradient accumulation, mixed-precision training, and evaluation on the validation dataset.\n",
    "\n",
    "* model = model: The Gemma 2B model with LoRA adapters applied in Cell 6, ready for parameter-efficient fine-tuning.\n",
    "\n",
    "* tokenizer = tokenizer: Ensures that all input prompts are tokenized using Gemma‚Äôs chat template for proper instruction-following behavior.\n",
    "\n",
    "* train_dataset / eval_dataset: The datasets preprocessed with format_chat_template in Cell 5, where each example is formatted as a clear instruction-response pair.\n",
    "\n",
    "* dataset_text_field = \"text\": Specifies the column in the dataset containing the formatted prompts and responses.\n",
    "\n",
    "* max_seq_length = max_seq_length: Ensures all sequences are truncated or padded to a consistent length for stable training.\n",
    "\n",
    "* dataset_num_proc = 2: Uses 2 parallel processes to speed up dataset preprocessing.\n",
    "\n",
    "* packing = False: Disables sequence packing; useful for dialogue-style sequences where prompts and responses should remain separate.\n",
    "\n",
    "* args = training_args: Passes the hyperparameters defined in Cell 7, which are optimized for memory-efficient, stable fine-tuning with LoRA.\n",
    "\n",
    "* trainer.train(): Starts the fine-tuning process, handling all forward and backward passes, gradient accumulation, optimizer steps, and mixed-precision computations automatically.\n",
    "\n",
    "This setup ensures that Gemma 2B learns the instruction-following task efficiently, while keeping GPU memory usage low and maintaining training stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c39a7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8\n",
    "\n",
    "\n",
    "\n",
    "print(\"üöÄ Initializing the SFTTrainer...\")\n",
    "\n",
    "# Initialize the trainer for LoRA fine-tuning\n",
    "trainer = SFTTrainer(\n",
    "    model=model,                             # Gemma 2B model with LoRA adapters\n",
    "    tokenizer=tokenizer,                     # Corresponding tokenizer\n",
    "    train_dataset=train_dataset_formatted,   # Preprocessed training dataset\n",
    "    eval_dataset=validation_dataset_formatted, # Preprocessed validation dataset\n",
    "    dataset_text_field=\"text\",               # Field containing the formatted prompt text\n",
    "    max_seq_length=max_seq_length,           # Maximum sequence length\n",
    "    dataset_num_proc=2,                      # Number of processes for dataset preprocessing\n",
    "    packing=False,                           # Disable packing for dialogue tasks; can enable for very short sequences\n",
    "    args=training_args,                      # Training arguments defined in Cell 7\n",
    ")\n",
    "\n",
    "# Start the fine-tuning process\n",
    "print(\"Starting model training...\")\n",
    "trainer_stats = trainer.train()\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b128461e",
   "metadata": {},
   "source": [
    "**Cell 9: Save Fine-Tuned LoRA Adapters and Tokenizer**\n",
    "\n",
    "After completing the LoRA fine-tuning, it is crucial to save the trained adapters and tokenizer for future use, such as inference, further fine-tuning, or deployment. This ensures that the model can be reloaded exactly as it was trained, maintaining the instruction-following behavior.\n",
    "\n",
    "* output_directory: Specifies the folder where the model adapters and tokenizer will be saved. Using a descriptive name helps identify the model and task.\n",
    "\n",
    "* os.makedirs(output_directory, exist_ok=True): Creates the directory if it doesn‚Äôt already exist, preventing errors during saving.\n",
    "\n",
    "* model.save_pretrained(output_directory): Saves the LoRA adapters (parameter-efficient fine-tuning weights) to the directory. If LoRA adapters are not applied, it saves the full model weights.\n",
    "\n",
    "* tokenizer.save_pretrained(output_directory): Saves the tokenizer configuration and vocabulary, ensuring consistent tokenization when the model is later loaded for inference.\n",
    "\n",
    "* print statements: Confirm that the adapters and tokenizer have been saved successfully, providing clear feedback for the user.\n",
    "\n",
    "This step finalizes the training workflow, producing a ready-to-use fine-tuned Gemma 2B model that can generate SOAP notes from medical dialogues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f214f8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9\n",
    "\n",
    "\n",
    "\n",
    "# Define the output directory\n",
    "output_directory = \"lora-fine-tuned-gemma-2b-medical-dialogue-to-soap-summary\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Save only the LoRA adapters (parameter-efficient fine-tuning)\n",
    "if hasattr(model, \"peft_config\"):\n",
    "    print(\"Saving LoRA adapters...\")\n",
    "    model.save_pretrained(output_directory)\n",
    "else:\n",
    "    print(\"No LoRA adapters found. Saving full model instead.\")\n",
    "    model.save_pretrained(output_directory)\n",
    "\n",
    "# Save the tokenizer\n",
    "print(\"Saving tokenizer...\")\n",
    "tokenizer.save_pretrained(output_directory)\n",
    "\n",
    "print(f\"Model adapters and tokenizer successfully saved to '{output_directory}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967ebe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip the finetuned model\n",
    "\n",
    "\n",
    "\n",
    "# Define the path to the model directory and the zip output path\n",
    "model_dir = '/kaggle/working/lora-fine-tuned-gemma-2b-medical-dialogue-to-soap-summary'\n",
    "zip_output = '/kaggle/working/lora_fine_tuned_model.zip'\n",
    "\n",
    "# Create a zip archive of the model directory\n",
    "shutil.make_archive(zip_output.replace('.zip', ''), 'zip', model_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa14a8d",
   "metadata": {},
   "source": [
    "**Cell 10: Evaluate Baseline Gemma 2B Model**\n",
    "\n",
    "In this cell, we evaluate the original, unfine-tuned Gemma 2B model on a sample from the test dataset. This provides a baseline performance to compare against the fine-tuned LoRA model, helping us understand how much improvement the instruction fine-tuning has achieved.\n",
    "\n",
    "* base_model, base_tokenizer: Loads the original Gemma 2B model and tokenizer in 4-bit precision for memory-efficient evaluation.\n",
    "\n",
    "* base_model.to(\"cuda\") & base_model.eval(): Moves the model to GPU and sets it to evaluation mode, disabling dropout and other training behaviors.\n",
    "\n",
    "* num_samples = 1: Specifies how many test samples to generate summaries for. Can be increased for more comprehensive evaluation.\n",
    "\n",
    "* messages: Formats the user input as a clear instruction prompt (‚ÄúSummarize the following medical dialogue into a SOAP note‚Äù) to align with the model‚Äôs instruction-following capability.\n",
    "\n",
    "* prompt = base_tokenizer.apply_chat_template(...): Converts the messages into the proper chat template expected by Gemma 2B, ensuring the model receives input in the same structured format as during training.\n",
    "\n",
    "* inputs & attention_mask: Tokenizes the prompt and generates an attention mask to tell the model which tokens to pay attention to.\n",
    "\n",
    "* base_model.generate(...): Produces the summary for the given dialogue using the baseline model.\n",
    "\n",
    "* baseline_summary = base_tokenizer.batch_decode(...): Decodes the generated token IDs into readable text, skipping special tokens.\n",
    "\n",
    "* Print statements: Display the original dialogue, reference SOAP summary, and the generated summary from the baseline model for easy side-by-side comparison.\n",
    "\n",
    "This evaluation step is essential to measure the improvement after fine-tuning, giving a clear quantitative and qualitative comparison between the pretrained baseline and the LoRA-adapted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443049fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10\n",
    "\n",
    "\n",
    "\n",
    "# Load the original base Gemma 2B model and tokenizer for baseline comparison\n",
    "print(\"Loading the original base Gemma 2B model...\")\n",
    "base_model, base_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/gemma-2b-it-bnb-4bit\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    trust_remote_code=True,  # Ensure custom model code is loaded\n",
    ")\n",
    "base_model.to(\"cuda\")\n",
    "base_model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Number of test samples to evaluate\n",
    "num_samples = 1  # Adjust as needed\n",
    "\n",
    "for i in range(num_samples):\n",
    "    # Select a sample from the test dataset\n",
    "    sample = test_dataset[i]\n",
    "    dialogue = sample[\"dialogue\"]\n",
    "    reference_summary = sample[\"soap\"]\n",
    "\n",
    "    # Format the messages using the chat template\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Summarize the following medical dialogue into a SOAP note:\\n\\n{dialogue}\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Format prompt using the tokenizer's chat template\n",
    "    prompt = base_tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True  # Ensures the model knows to generate a response\n",
    "    )\n",
    "\n",
    "    # Tokenize the prompt and move inputs to GPU\n",
    "    inputs = base_tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Create attention mask\n",
    "    attention_mask = (inputs[\"input_ids\"] != base_tokenizer.pad_token_id).to(torch.long)\n",
    "\n",
    "    # Generate output from the baseline (unfine-tuned) model\n",
    "    print(f\"\\nGenerating summary for sample {i + 1} (BASE model)...\")\n",
    "    outputs = base_model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id=base_tokenizer.eos_token_id,\n",
    "        pad_token_id=base_tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "    # Decode the generated summary\n",
    "    baseline_summary = base_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    generated_summary = baseline_summary[0]\n",
    "\n",
    "    # Print results for comparison\n",
    "    print(\"\\n--- BASELINE MODEL (NO FINE-TUNING) ---\")\n",
    "    print(\"\\nDIALOGUE:\")\n",
    "    print(dialogue)\n",
    "    print(\"\\nREFERENCE SOAP SUMMARY:\")\n",
    "    print(reference_summary)\n",
    "    print(\"\\nGENERATED SUMMARY (BASELINE):\")\n",
    "    print(generated_summary)\n",
    "    print(\"\\n--- END OF BASELINE ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f671dfb6",
   "metadata": {},
   "source": [
    "**Cell 11: Evaluate Fine-Tuned Gemma 2B Model**\n",
    "\n",
    "In this cell, we evaluate the fine-tuned Gemma 2B model on a sample from the test dataset. This allows us to see how well the model generates SOAP notes after LoRA-based instruction fine-tuning, and to compare it with the baseline model.\n",
    "\n",
    "* fine_tuned_model, fine_tuned_tokenizer: Loads the Gemma 2B model fine-tuned on medical dialogues and the corresponding tokenizer. Using load_in_4bit ensures memory-efficient loading.\n",
    "\n",
    "* fine_tuned_model.to(\"cuda\") & fine_tuned_model.eval(): Moves the model to GPU and sets evaluation mode to disable dropout, ensuring deterministic outputs.\n",
    "\n",
    "* num_samples = 1: Specifies how many test samples to evaluate; can be increased for more extensive testing.\n",
    "\n",
    "* messages: Formats the user input as a clear instruction prompt, telling the model to summarize the dialogue into a SOAP note.\n",
    "\n",
    "* prompt = fine_tuned_tokenizer.apply_chat_template(...): Converts the messages into the proper chat template expected by Gemma 2B, maintaining consistency with training.\n",
    "\n",
    "* inputs & attention_mask: Tokenizes the prompt and creates an attention mask, indicating which tokens the model should attend to.\n",
    "\n",
    "* torch.no_grad(): Disables gradient computation to save memory and speed up inference since we are only generating outputs.\n",
    "\n",
    "* fine_tuned_model.generate(...): Generates the SOAP note for the given dialogue, using max_new_tokens to control the output length and properly handling EOS and padding tokens.\n",
    "\n",
    "* fine_tuned_tokenizer.batch_decode(...): Decodes the generated tokens into readable text. The assistant‚Äôs response is extracted with .split(\"assistant\\n\")[-1].strip() to ensure only the generated SOAP note is evaluated.\n",
    "\n",
    "* Print statements: Display the original dialogue, reference SOAP summary, and the generated summary from the fine-tuned model, making it easy to compare qualitative improvements over the baseline.\n",
    "\n",
    "This step demonstrates the effectiveness of LoRA fine-tuning and provides concrete examples of how the model performs on real medical dialogues, forming the basis for both qualitative and quantitative evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9887debc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11\n",
    "\n",
    "# Load the fine-tuned Gemma 2B model and tokenizer\n",
    "print(\"Loading the fine-tuned model for evaluation...\")\n",
    "fine_tuned_model, fine_tuned_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"aslamsikder/lora-fine-tuned-gemma-2b-medical-dialogue-to-soap-summary\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "fine_tuned_model.eval()  # Set to evaluation mode\n",
    "\n",
    "num_samples = 1  # Adjust as needed\n",
    "\n",
    "for i in range(num_samples):\n",
    "    sample = test_dataset[i]\n",
    "    dialogue = sample[\"dialogue\"]\n",
    "    reference_summary = sample[\"soap\"]\n",
    "\n",
    "    # Prepare chat messages\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Summarize the following medical dialogue into a SOAP note perfectly:\\n\\n{dialogue}\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    prompt = fine_tuned_tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = fine_tuned_tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    attention_mask = (inputs[\"input_ids\"] != fine_tuned_tokenizer.pad_token_id).to(torch.long)\n",
    "\n",
    "    print(f\"\\nGenerating summary for sample {i + 1} (FINE-TUNED model)...\")\n",
    "    with torch.no_grad():\n",
    "        outputs = fine_tuned_model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=512,\n",
    "            eos_token_id=fine_tuned_tokenizer.eos_token_id,\n",
    "            pad_token_id=fine_tuned_tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "    # Decode and clean output\n",
    "    fine_tuned_summary = fine_tuned_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    raw_output = fine_tuned_summary[0]\n",
    "\n",
    "    # Remove assistant/user prefixes and unwanted artifacts\n",
    "    cleaned_output = re.sub(r'^(user|model|assistant)\\n', '', raw_output, flags=re.IGNORECASE)\n",
    "    cleaned_output = re.sub(r'strtoA:.*', '', cleaned_output) \n",
    "    cleaned_output = cleaned_output.strip()\n",
    "\n",
    "    print(\"\\n--- FINE-TUNED MODEL (CLEANED) ---\")\n",
    "    print(\"\\nDIALOGUE:\")\n",
    "    print(dialogue)\n",
    "    print(\"\\nREFERENCE SOAP SUMMARY:\")\n",
    "    print(reference_summary)\n",
    "    print(\"\\nGENERATED SUMMARY (FINE-TUNED):\")\n",
    "    print(cleaned_output)\n",
    "    print(\"\\n--- END OF FINE-TUNED ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3852615c",
   "metadata": {},
   "source": [
    "**Cell 12: Quantitative Evaluation on the Test Set**\n",
    "\n",
    "In this cell, we evaluate the performance of the fine-tuned Gemma 2B model on the test dataset using standard text summarization metrics: ROUGE and BERTScore. This provides an objective measure of how well the model generates SOAP notes from medical dialogues.\n",
    "\n",
    "* predictions / references: Lists to store the generated summaries and their corresponding ground-truth SOAP notes.\n",
    "\n",
    "* messages: Formats the user input as a clear instruction (‚ÄúSummarize the following medical dialogue into a SOAP note‚Äù) using the chat template, ensuring consistency with training.\n",
    "\n",
    "* tokenizer.apply_chat_template(...): Converts the messages into the proper prompt format expected by Gemma 2B.\n",
    "\n",
    "* inputs & attention_mask: Tokenizes the prompt and generates the attention mask, telling the model which tokens to attend to during generation.\n",
    "\n",
    "* torch.no_grad(): Disables gradient calculation to save memory and speed up evaluation since no backpropagation is required.\n",
    "\n",
    "* model.generate(...): Generates the summary for each dialogue. max_new_tokens limits the summary length, while eos_token_id and pad_token_id ensure proper sequence termination and padding handling.\n",
    "\n",
    "* tokenizer.batch_decode(...): Decodes token IDs into readable text, skipping special tokens. The assistant‚Äôs response is extracted using .split(\"assistant\\n\")[-1].strip().\n",
    "\n",
    "* ROUGE: Evaluates overlap between generated summaries and reference SOAP notes, capturing precision, recall, and F1 for n-grams.\n",
    "\n",
    "* BERTScore: Measures semantic similarity between predictions and references using contextual embeddings; the average F1 score provides a single overall metric.\n",
    "\n",
    "* Print results: Displays ROUGE and BERTScore in a readable percentage format for easy comparison.\n",
    "\n",
    "This evaluation step provides a quantitative benchmark to assess how much the fine-tuning improved the model over the baseline and helps validate the quality of generated SOAP notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba9bd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12\n",
    "\n",
    "# Load evaluation metrics\n",
    "rouge = evaluate.load('rouge')\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "# Initialize lists to store predictions and references\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "print(\"\\nRunning quantitative evaluation on the test set...\")\n",
    "\n",
    "# Loop through the entire test dataset\n",
    "for sample in tqdm(test_dataset):\n",
    "    dialogue = sample[\"dialogue\"]\n",
    "    reference = sample[\"soap\"]\n",
    "\n",
    "    # Format prompt for model using chat template\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Summarize the following medical dialogue into a SOAP note:\\n\\n{dialogue}\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True  # Ensures the model generates an assistant response\n",
    "    )\n",
    "\n",
    "    # Tokenize and move to GPU\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Generate summary from the fine-tuned model\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=(inputs[\"input_ids\"] != tokenizer.pad_token_id).to(torch.long),\n",
    "            max_new_tokens=512,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "    # Decode generated text\n",
    "    generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    # Extract the assistant response\n",
    "    prediction = generated_text[0].split(\"assistant\\n\")[-1].strip()\n",
    "\n",
    "    # Append to lists\n",
    "    predictions.append(prediction)\n",
    "    references.append(reference)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "rouge_results = rouge.compute(predictions=predictions, references=references)\n",
    "bertscore_results = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "\n",
    "# Print results\n",
    "print(\"\\n--- QUANTITATIVE EVALUATION RESULTS ---\")\n",
    "print(\"\\nROUGE Scores:\")\n",
    "for key, value in rouge_results.items():\n",
    "    print(f\"{key}: {value*100:.2f}\")\n",
    "\n",
    "print(\"\\nBERTScore:\")\n",
    "# Use mean F1 score for summary evaluation\n",
    "avg_bert_f1 = sum(bertscore_results['f1']) / len(bertscore_results['f1'])\n",
    "print(f\"Average F1 Score: {avg_bert_f1*100:.2f}\")\n",
    "print(\"\\n--- END OF EVALUATION ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2914892",
   "metadata": {},
   "source": [
    "**Cell 13: Deploy Fine-Tuned Gemma 2B Model via Flask API**\n",
    "\n",
    "This cell sets up a Flask web API to allow remote inference with your fine-tuned Gemma 2B model. It provides a simple endpoint to submit medical dialogues and receive generated SOAP summaries, making the model accessible for web applications or integration with other tools.\n",
    "\n",
    "* Flask app initialization: app = Flask(__name__) creates the Flask application instance.\n",
    "\n",
    "* Model loading (outside request handler):\n",
    "\n",
    "- Loads the fine-tuned Gemma 2B model and tokenizer once to avoid repeated loading on each request.\n",
    "\n",
    "- load_in_4bit=True ensures memory-efficient loading.\n",
    "\n",
    "- model.to(\"cuda\") moves the model to GPU for faster inference.\n",
    "\n",
    "- model.eval() disables dropout and other training-specific layers for deterministic outputs.\n",
    "\n",
    "- FastLanguageModel.for_inference(model) optimizes the model for inference.\n",
    "\n",
    "* /summarize endpoint (POST):\n",
    "\n",
    "- Accepts JSON input containing a \"dialogue\" field.\n",
    "\n",
    "- Returns JSON output with the generated SOAP summary.\n",
    "\n",
    "* Prompt formatting:\n",
    "\n",
    "- The user input is converted into the Llama 3 chat template (messages ‚Üí prompt) to match the fine-tuning format.\n",
    "\n",
    "- add_generation_prompt=True ensures the model knows it should generate an assistant response.\n",
    "\n",
    "* Tokenization and attention mask:\n",
    "\n",
    "- The prompt is tokenized and moved to GPU.\n",
    "\n",
    "- The attention mask identifies which tokens are real versus padding, ensuring proper handling during generation.\n",
    "\n",
    "* Text generation:\n",
    "\n",
    "- torch.no_grad() disables gradient computation for memory efficiency.\n",
    "\n",
    "- model.generate(...) produces the summary with controlled length (max_new_tokens) and correct EOS/pad token handling.\n",
    "\n",
    "* Extract assistant response: generated_text[0].split(\"assistant\\n\")[-1].strip() ensures only the generated SOAP note is returned.\n",
    "\n",
    "* Error handling: Returns descriptive JSON errors if the model isn‚Äôt loaded, the input is invalid, or generation fails.\n",
    "\n",
    "* Running the app: app.run(host='0.0.0.0', port=7860) starts the server accessible on all network interfaces; 7860 is compatible with Hugging Face Spaces.\n",
    "\n",
    "This setup provides a production-ready, lightweight API for generating SOAP notes from medical dialogues using the fine-tuned Gemma 2B model, making it easy to integrate into web apps or other services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b087abb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13\n",
    "\n",
    "\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# --- Model Loading (Load once, outside request handler) ---\n",
    "try:\n",
    "    max_seq_length = 2048\n",
    "    dtype = None\n",
    "    load_in_4bit = True\n",
    "\n",
    "    # Load the fine-tuned Gemma 2B model and tokenizer\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"aslamsikder/fine-tuned-gemma-2b-medical-dialogue-to-soap-summary\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "        trust_remote_code=True  # Ensures custom Hugging Face code is loaded\n",
    "    )\n",
    "    model.to(\"cuda\")\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    print(\"Model loaded successfully for inference.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    model, tokenizer = None, None\n",
    "# --- End Model Loading ---\n",
    "\n",
    "@app.route('/summarize', methods=['POST'])\n",
    "def summarize():\n",
    "    if not model or not tokenizer:\n",
    "        return jsonify({\"error\": \"Model is not loaded\"}), 500\n",
    "\n",
    "    # Parse JSON input\n",
    "    data = request.get_json()\n",
    "    if not data or 'dialogue' not in data:\n",
    "        return jsonify({\"error\": \"Invalid input. 'dialogue' key is required.\"}), 400\n",
    "\n",
    "    dialogue = data['dialogue']\n",
    "\n",
    "    # Format prompt using Llama 3 chat template\n",
    "    messages = [{\"role\": \"user\", \"content\": dialogue}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Tokenize and move inputs to GPU\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Generate summary\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=(inputs[\"input_ids\"] != tokenizer.pad_token_id).to(torch.long),\n",
    "                max_new_tokens=512,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "\n",
    "        generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        # Extract assistant response\n",
    "        summary = generated_text[0].split(\"assistant\\n\")[-1].strip()\n",
    "\n",
    "        return jsonify({\"summary\": summary})\n",
    "\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": f\"Error during generation: {str(e)}\"}), 500\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # For production, use a WSGI server like Gunicorn; port 7860 is default for HF Spaces\n",
    "    app.run(host='0.0.0.0', port=7860)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c71d8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FineTuneLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
